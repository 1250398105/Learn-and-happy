#面试题整理

## 1，java部分：

### 1.1 问内存中有 1000 0000 条数据，分别用 interger,int 类型会有什么差别

答：首先要明白interger,int之间的区别

1、Integer是int的包装类，int则是java的一种基本数据类型 

2、Integer变量必须实例化后才能使用，而int变量不需要 

3、Integer实际是对象的引用，当new一个Integer时，实际上是生成一个指针指向此对象；而int则是直接存储数据值 。

4、Integer的默认值是null，int的默认值是0

==**内存中放那么大的数据，integer对-128~127的数进行缓存，直接从常量池中取数据，不会去new 新对象，但是超过这个数就会产生新的对象，所有堆内存会被大量占用。**==



###1.2 HashMap底层实现原理，ConcurrentHashMap和HashTable三者区别

答：

==java1.8版本（HashMap）：==

1. 底层：数组+红黑树/链表 =>(==线程不安全的==)
2. 数组里面存放的数据类型为node，初始值为null
3. 数组初始容量为16
4. 数组加载因子为0.75
5. 数组最小扩容为12的时候扩容
6. 数组扩容为原来的2倍
7. 第一次树化阈值：数组容量>=64,并且链表>=8
8. 反树化阈值：链表<=6的时候
9. 链表新添加的元素放到链表的尾部（七上八下）

==java1.7版本（concurrenthashmap）：==

1. 不允许将 **`null`** 用作键或值
2. 遵守与 **`Hashtable`**相同的功能规范
3. **默认的并发级别为 16，也就是说默认创建 16 个 Segment。**
4. **锁分段技术（把Map分成了N个Segment，根据key.hashCode()算出放到哪个Segment中，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。）**


==java1.8版本（concurrenthashmap）：==

1. java1.8版本中放弃了Segment臃肿的设计，取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现

2. 1.8中使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据或则删除数据时，会通过addCount()方法更新baseCount，通过累加baseCount和CounterCell数组中的数量，即可得到元素的总个数；

   ​

   ​


==java1.8版本（HashTable）：==

1. 是线程安全的
2. 不允许将 **`null`** 用作键或值
3. 使用**`synchronized`**同步方法，锁住一个方法，同一时间只有一个线程进行操作，效率性能低下。


### 1.3 ArrayList、HashSet底层实现原理，线程安全问题怎么解决？

答：

==java1.8版本（ArrayList）：==

1. 底层结构是数组

2. 初始容量为0,第一次添加元素之间初始为10.

3. 扩容为原来的1.5倍（用户不可指定）,第一次扩容为15，第二次为22。

4. 底层怎么扩容的，Arrays.copyof() <==> (浅拷贝,只拷贝指针，相当于两个数组指向同一块内存)

5. 线程不安全，使用多线程的时候会出现ConcurrentModificationException异常：并发修改异常

   **怎么解决线程不安全？**

   + 使用vectore解决线程安全的，采用的是synchronized关键字
   + 使用集合工具类，来解决线程安全问题，collections.synchronizedList(new ArrayList<>())
   + 使用new CopyOnWriteArrayList()来解决，写时复制技术(读写分离)，先拷贝一份进行修改，原来的那份可以提供读服务，底层实现是用lock同步锁，==适合场景：适合写少读多==，缺点：每次修改都会拷贝新数组，产生gc，内存消耗。

==java1.8版本（HashSet）：==

1. 底层实现结构是HashMap，HashMap的key作为HashSet的值
2. 线程不安全
3. 怎么解决线程安全问题：
   + 使用集合工具类来操作：collections.synchronizedList(new HashSet<>())
   + 使用new CopyOnWriteArrayList()来解决。


### 1.4 说说遇见到的五个常用异常

答：

1. NullPointExecption异常：空指针异常

> 原因：
>
> ​	因为一个对象的引用值为null，然后调用这个对象的属性和方法，会报空指针异常。
>
> 解决方案：
>
> 	1. Optional是一个容器对象,用来解决令所有程序员头疼的空指针问题，Optional.empty()：所有null包装成的Optional对象：
> 	2. 使用if来判断是否为null，来预防空指针异常

2. Tasknotserializable异常：task没有系列化异常

> 原因：
>
> ​	driver定义的变量或者数据要传输到exerctor进行执行，要经过网络传输一定要序列化，但是有时候这种方式解决不了，java类很多序列化的类被关键字transfrom限制不会系列化。
>
> 解决异常：
>
> 	1. 这个类实现Java的serlizable方法方法
> 	2. 使用广播变量来解决这个问题，广播变量里面是使用kyro序列号方式，spark自带的系列化方式。

3. ConcurrentModificationException异常：并发修改异常

> 原因：
>
> 
>
> 解决方案：
>
> 

4. StackOverflowError异常：栈溢出异常(错误)

> 原因：
>
> 
>
> 解决方案：
>
> 

5. OutOfMemoryError异常：堆溢出异常(错误)

> 原因：
>
> 
>
> 解决方案：
>
> 	1. 合理的XX: PermSize和-XX:MaxPermSize参数值
> 	2. 

### 1.5 请你解释为什么重写equals还要重写hashcode？

答：

==**Object类默认的equals比较规则就是比较两个对象的内存地址。而hashcode是native修饰本地方法，hashcode是根据对象的内存地址经哈希算法得来的。**==

现在有两个Student对象：

    Student s1=new Student("小明",18);

    Student s2=new Student("小明",18);

此时s1.equals(s2)一定返回true

假如只重写equals而不重写hashcode，那么Student类的hashcode方法就是Object默认的hashcode方法，由于默认的hashcode方法是根据对象的内存地址经哈希算法得来的，显然此时s1!=s2,故两者的hashcode不一定相等。

然而重写了equals，且s1.equals(s2)返回true，根据hashcode的规则，两个对象相等其哈希值一定相等，所以矛盾就产生了，因此重写equals一定要重写hashcode，而且从Student类重写后的hashcode方法中可以看出，重写后返回的新的哈希值与Student的两个属性有关。

以下是关于hashcode的一些规定：

➀相等（相同）的对象必须具有相等的哈希码（或者散列码）。

➁如果两个对象的hashCode相同，它们并不一定相同。



### 1.6 请说说快速失败(fail-fast)和安全失败(fail-safe)的区别？

答：

Iterator的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。java.util包下面的所有的集合类都是快速失败的，而java.util.concurrent包下面的所有的类都是安全失败的。快速失败的迭代器会抛出ConcurrentModificationException异常，而安全失败的迭代器永远不会抛出这样的异常。

### 1.7 请你解释HashMap的容量为什么是2的n次幂？

答：https://blog.csdn.net/eaphyy/article/details/84386313

> 看源码，我们可以发现，确定数组位置的实现是 **==i=（n-1）& hash==**，其中 n 代表数组的长度，即map的容量。
>
> 当n为2的幂次方时，（n-1）& hash 的值是均匀分布的，我们假设n=16，hash从0开始递增：
>
> **hash	（n-1）& hash	结果**
> **0	1111 & 0	0**
> **1	1111 & 1	1**
> **2	1111 & 10	2**
> **3	1111 & 11	3**
> **4	1111 & 100	4**
> **5	1111 & 101	5**
> **……	……	……**
> **16	1111 & 10000	0**
> **17	1111 & 10001	1**
>
> 
>
> 18	1111 & 10010	2
>
> 当n不为2的幂次方时，（n-1）& hash 的值不是是均匀分布的，我们假设n=15，hash从0开始递增：
>
> **hash	（n-1）& hash	结果**
> **0	1110 & 0	0**
> **1	1110 & 1	0**
> **2	1110 & 10	2**
> **3	1110 & 11	2**
> **4	1110 & 100	4**
> **5	1110 & 101	4**
> **……	……	……**
> **16	1110 & 10000	0**
> **17	1110 & 10001	0**
> **18	1110 & 10010	2**
> 由上面可以看出，当我们根据key的hash确定其在数组的位置时，如果n为2的幂次方，可以保证数据的均匀插入，如果n不是2的幂次方，可能数组的一些位置永远不会插入数据，浪费数组的空间，加大hash冲突。
> 另一方面，一般我们可能会想通过 % 求余来确定位置，这样也可以，只不过性能不如 & 运算。而且当n是2的幂次方时：hash & (length - 1) == hash % length
>
> 因此，HashMap 容量为2次幂的原因，就是为了数据的的均匀分布，减少hash冲突，毕竟hash冲突越大，代表数组中一个链的长度越大，这样的话会降低hashmap的性能。

## 2，Scala部分：

###2.1 谈谈对特质的理解

答：

> scala里面和java里面的接口很相似，但是Scala中的特质可以同时拥有抽象方法和具体方法，而类可以==实现(用的关键字的extends)==多个特质,当需要的特质不止一个的时候，可以使用==with关键字==来添加其它的特质。
>
> ==构造器执行顺序，如下所示：==
>
> 首先调用超类的构造器；特质构造器在超类构造器之后，类构造器之前执行；特质由左到右被构造；每个特质当中，父特质先被构造；如果多个特质共有一个父特质，而那个父特质已经被构造，则不会被再次构造；所有特质构造完毕，子类被构造。

**E:**举个例子来说明，如下所示：

```scala
class SavingsAccount extends Account with FileLogger with ShortLogger
```

构造器执行顺序，如下所示：

==Account（超类）；Logger（第一个特质的父特质）；FileLogger（第一个特质）；ShortLogger（第二个特质）；SavingsAccount（类）==。

特别强调的是，==特质不能有构造器参数，每个特质都有一个无参数的构造器==，由字段的初始化和其它特质体中的语句构成。

### 2.2 函数科里化

答：



## 3，Hbase部分：

### 3.1 HBASE版本

答：版本号是1.3.1

### 3.2 



## 4，Hive部分：

###4.1用 hive 对一张表做分组排序 topN，然后谈一下优化策略

答：建表语句：

```sql
CREATE table datatable (
  country string,
  city string,
  Visitors int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE;
```

==需求：按照国家、城市取TOP3==

```sql
-- 取Top3,先分析，按照国家排序
select a.*
from (
select country,city,visitors, row_number() over (partition by country order by visitors desc ) rank  
from tripdata
order by country,visitors desc
) a
where a.rank<=3;
```

## 5，Hadoop部分：

###5.1 Spark 与hadoop 对比，有哪些优势?

答：

1）Spark相比Hadoop在处理模型上的优势

首先，Spark 摒弃了MapReduce 先 map 再 reduce这样的严格方式，Spark引擎执行更通用的有向无环图（DAG）算子。

另外，基于MR的计算引擎在shuffle过程中会将中间结果输出到磁盘上，进行存储和容错，而且HDFS的可靠机制是将文件存为3份。Spark是将执行模型抽象为通用的有向无环图执行计划(DAG),当到最后一步时才会进行计算，这样可以将多stage的任务串联或者并行执行，而无须将stage中间结果输出到HDFS。磁盘IO的性能明显低于内存，所以Hadoop的运行效率低于spark。

2）数据格式和内存布局

MR在读的模型处理方式上会引起较大的处理开销，spark抽象出弹性分布式数据集RDD，进行数据的存储。RDD能支持粗粒度写操作，但对于读取操作，RDD可以精确到每条记录，这使得RDD可以用来作为分布式索引。Spark的这些特性使得开发人员能够控制数据在不同节点上的不同分区，用户可以自定义分区策略，如hash分区等。

3）执行策略

MR在数据shuffle之前花费了大量的时间来排序，spark可减轻这个开销。因为spark任务在shuffle中不是所有的场合都需要排序，所以支持基于hash的分布式聚合，调度中采用更为通用的任务执行计划图(DAG)，每一轮次的输出结果都在内存中缓存。

4）任务调度的开销

传统的MR系统，是为了运行长达数小时的批量作业而设计的，在某些极端情况下，提交一个任务的延迟非常高。Spark采用了时间驱动的类库AKKA来启动任务，通过线程池复用线程来避免进程或线程启动和切换开销。

5）内存计算能力的扩展

spark的弹性分布式数据集（RDD）抽象使开发人员可以将处理流水线上的任何点持久化存储在跨越集群节点的内存中，来保证后续步骤需要相同数据集时就不必重新计算或从磁盘加载，大大提高了性能。这个特性使Spark 非常适合涉及大量迭代的算法，这些算法需要多次遍历相同数据集， 也适用于反应式（reactive）应用，这些应用需要扫描大量内存数据并快速响应用户的查询。

6）开发速度的提升

构建数据应用的最大瓶颈不是 CPU、磁盘或者网络，而是分析人员的生产率。所以spark通过将预处理到模型评价的整个流水线整合在一个编程环境中， 大大加速了开发过程。Spark 编程模型富有表达力，在 REPL 下包装了一组分析库，省去了多次往返 IDE 的开销。而这些开销对诸如 MapReduce 等框架来说是无法避免的。Spark 还避免了采样和从HDFS 来回倒腾数据所带来的问题，这些问题是 R 之类的框架经常遇到的。分析人员在数据上做实验的速度越快，他们能从数据中挖掘出价值的可能性就越大。



### 5.2 手写mr实现两张表之间的join操作







## 6，kafka部分：

### 6.1 kafka 的高并发是如何实现的

答：kafka主要使用了以下几个方式实现了超高的吞吐率

1. 顺序读写:
2. 零拷贝:
3. 文件分段：
4. 批量发送：
5. 数据压缩：

https://aoyouzi.iteye.com/blog/2322673

### 6.2 kafka 实际开发遇到的问题

答：

> ==**把下面的问题总结一句话：**==
>
> **kafka默认是自动提交的，但是这样不安全**
>
> ==自动提交：==就是说消费者拉取到数据后直接提交offset，不管业务逻辑场景，可能处理有事务控制的任务，往msql写数据，一旦发生错误，数据回滚，但是offset已经提交，就会产生数据丢失。
>
> ==手动提交：==当我们拉取到数据后，kafak会放到队列里面，等我们处理完数据之后，没有错误，就会把offset提交，但是中间一旦发生错误，会马上回滚数据。



==**丢包问题**==

丢包问题：消息推送服务，每天早上，手机上各终端都会给用户推送消息，这时候流量剧增，可能会出现kafka发送数据过快，导致服务器网卡爆满，或者磁盘处于繁忙状态，可能会出现丢包现象。

**解决办法**

> 限速，启用重试机制，重试间隔时间设置长一些，Kafka设置acks = -1
>
> 首先==对kafka进行限速， 其次启用重试机制==，重试间隔时间设置长一些，最后Kafka设置acks = -1，即需要相应的所有处于ISR的分区都确认收到该消息后，才算发送成功。
>
> 检测方法：使用重放机制，查看问题所在。

==**重发问题**==

重发问题：当消费者重新分配partition的时候，可能出现从头开始消费的情况，导致重发问题。当消费者消费的速度很慢的时候，可能在一个session周期内还未完成，导致心跳机制检测报告出问题。

**解决办法**

> 底层根本原因：已经消费了数据，但是offset没提交。
>
> 配置问题：设置了offset自动提交
>
> ==解决办法：将enable.auto.commit设置成false，即不采用自动提交方式；==
>
> 问题场景：
>
> 1.设置offset为自动提交，正在消费数据，kill消费者线程；
>
> 2.设置offset为自动提交，关闭kafka时，如果在close之前，调用 consumer.unsubscribe() 则有可能部分offset没提交，下次重启会重复消费；
>
> 3.消费kafka与业务逻辑在一个线程中处理，可能出现消费程序业务处理逻辑阻塞超时，导致一个周期内，offset还未提交；继而重复消费，但是业务逻辑可能采用发送kafka或者其他无法回滚的方式

==**消费者pull数据时，出现数据丢失（ACK机制）**==

- 自动提交offset：可能出现数据丢失，当消费者的数量>partition数量时，出现提交异常。。。多个consumer group 同时消费一个分区的数据，其中一个先提交了，另一个就丢失了。
- 手动提交offset : 防止线程安全问题。

**解决办法**

> 手动commit offset，并针对partition_num启同样数目的consumer进程，这样就能保证一个consumer进程占有一个partition，commit offset的时候不会影响别的partition的offset。但这个方法比较局限，因为partition和consumer进程的数目必须严格对应。
>
> 另一个方法同样==需要手动commit offset，另外在consumer端再将所有fetch到的数据缓存到queue里，当把queue里所有的数据处理完之后，再批量提交offset==，这样就能保证只有处理完的数据才被commit。当然这只是基本思路，实际上操作起来不是这么简单，具体做法以后我再另开一篇。



### 6.3 kafka 工作机制（kafka 数据流程和分区，ack,ISR 都可以介绍）

答：



### 6.4 kafka怎么重复消费怎么解决

答：



### 6.5 Spark Streaming和Kafka整合是如何保证数据零丢失

答：

1. 可靠的数据源和可靠的接收器
2. ​



## 7，Spark部分：

### 7.1 spark 的工作机制（yarn）

![](img\spar的yarn运行流程.png)

==**本地化策略：**==

> 因为这里涉及到一个Driver会对每一个stage的task进行分配：
>
> 根据Spark的==task分配算法，Spark希望task能够运行在它要计算的数据算在的节点（数据本地化思想）==，这样就可以避免数据的网络传输。通常来说，task可能不会被分配到它处理的数据所在的节点，因为这些节点可用的资源可能已经用尽，此时，==Spark会等待一段时间，默认3s，如果等待指定时间后仍然无法在指定节点运行，那么会自动降级==，**进程本地化>节点本地化>机架本地化。**
>
> | **名称**              | **解析**                                                     |
> | --------------------- | ------------------------------------------------------------ |
> | ==**PROCESS_LOCAL**== | ==进程本地化==，task和数据在同一个Executor中，性能最好。     |
> | ==**NODE_LOCAL**==    | ==节点本地化==，task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输。 |
> | ==**RACK_LOCAL**==    | ==机架本地化==，task和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。 |
> | **NO_PREF**           | 对于task来说，从哪里获取都一样，没有好坏之分。               |
> | **ANY**               | task和数据可以在集群的任何地方，而且不在一个机架中，性能最差。 |
>
> 
>
> 



### 7.2 spark任务调度流程

![](img\spark任务调度.png)

###7.3 累加器原理，你用在项目哪里，解决什么问题，累加器陷阱了解吗？和广播变量有啥区别？

答：

>  ==**累加器是用来操作数据的，读写都可以。广播变量是只读的，用来调优使用的。**==

​	

==**累加器**==（accumulator）：==累加器主要用于多个节点对一个变量进行共享性的操作==,累加器是仅仅被相关操作累加的变量，因此可以在并行中被有效地支持。它们可用于实现==计数器==（如MapReduce）或==总和计数==。==Accumulator是存在于Driver端的==，集群上运行的task进行Accumulator的累加，随后把值发到Driver端，在Driver端汇总，由于Accumulator存在于Driver端，==从节点读取不到Accumulator的数值==。

> ​	**大概意思就是，操作rdd的算子的时候，可能需要传一个driver端的参数，但是最后要在driver展示。所以这个时候就要用到累加器了，把executor的数据全部聚合在一起，返回给driver端。**
>
> ​	**有时候可以代替reduceByKey，比如计算wordCount的时候，累加器比reduceByKey性能更好，因为累加器不需要经过shuffle。**

​    ==**广播变量**==允许编程者在==每个Executor上保留外部数据的只读变量==，而不是给每个任务发送一个副本。 

每个task都会保存一份它所使用的外部变量的副本，当一个Executor上的多个task都使用一个大型外部变量时，对于Executor内存的消耗是非常大的，因此，我们可以将大型外部变量封装为广播变量，此时一个Executor保存一个变量副本，此Executor上的所有task共用此变量，不再是一个task单独保存一个副本，这在一定程度上降低了Spark任务的内存占用。



### 7.4 reduceBykey 和 groupbykey 的区别 

答：

 E案例: 

```scala
scala> val rdd = sc.makeRDD(List(("java",1),("php",1),("java",2),("python",1),("scala",1)))
```

 reduceBykey 结果:（相同的key做聚合处理。）

```scala
scala> rdd.reduceByKey(_+_).collect
res3: Array[(String, Int)] = Array((php,1), (python,1), (scala,1), (java,3))
```

groupbykey 结果：(相同的key做分组，不做聚合。)

```scala
scala> rdd.groupByKey().collect
res7: Array[(String, Iterable[Int])] = Array((php,CompactBuffer(1)), (python,CompactBuffer(1)), (scala,CompactBuffer(1)), (java,CompactBuffer(1, 2)))
```

> 区别：
>
> ①： reduceBykey ：会产生shuffle，相同的key做聚合运算，聚合之前会进行combine合并，性能更		 优。
>
> ②：groupbykey ：会产生shuffle，相同的key做分组，不会聚合，相同的key对应的value放在一个CompactBuffer集合里面。

### 7.5 spark 的内存管理

答：

==堆内内存==受到JVM统一管理，GC不可以控制。

==堆外内存==是直接向操作系统进行内存的申请和释放（可以控制GC）。



**静态内存管理（1.6版本之前）：**

​	**堆内内存分配**：分为三个部分

​	![](img\静态内存-堆内内存.png)

​	**堆外内存：**

![](img\静态内存-堆外内存.png)



>  这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。

**动态内存管理（1.6版本之后）：**

​	**堆内内存分配**：

![](img\动态内存-堆内内存.png)

​	**堆外内存：**

![](img\动态内存-堆外内存.png)



> 总结：
>
> - 双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）
> - ==执行内存==的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后"归还"借用的空间
> - ==存储内存==的空间被对方占用后，无法让对方"归还"，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。

**堆外内存-动态占用机制：**

![](img\动态内存-堆内内存=占用机制.png)



### 7.6 spark的suffle哪几种，讲一讲各自的区别。

答：

> ==**未优化的HashSuffle:**==
>
> ==**每个Executor创建的磁盘文件的数量的计算公式为 : 当前task数量*下一个stage的task数量。**==
>
> > 将每个task处理的数据按key进行“划分”。所谓“划分”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。
>
> 例子：假设当前有
>
> ![](img\HashSuffle未优化.png)
>
> 
>
> ==**优化后的HashShffle**==
>
> ==**每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量**==
>
> ![](img\HashSuffle优化后.png)
>
> ==**SortShuffleManager（普通机制）：**==
>
> >一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。
> >
> >由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。
>
> ![](img\SortShuffleManager普通.png)
>
> ==**SortShuffle(Bypass模式)：**==
>
> ==**shuffle read task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值（默认为200）。**==
>
> > **第一，磁盘写机制不同:**
> >
> > ​	该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的。
> >
> > **第二，不会进行排序:**
> >
> > ​	不需要进行数据的排序操作，也就节省掉了这部分的性能开销。
>
> ![](img\SortShuffleManager-ByPass.png)





### 7.7 RDD,DataFrame和DataSet的区别

答：

==**RDD是分布式的数据集**==：没有结构，没有类型，内存不好控制，序列化和反系列化都需要把对象数据和对象结构全部进行序列化。

==**DataFrame是有结构的，类型是Row**==，还引入了off-heap（堆外内存），更好的控制内存，序列化和反序列化时只需要把数据进行序列化，结构不需要进行序列化。 

==**DataSet既有结构，也有类型，**==引入了编码器，可以做到按需进行序列化，而不是把数据全部进行序列化。

![](img\RDD-DataFrame-DataSet.png)

### 7.8 处理RDD状态的三种方式

答：

1. updatastatsByKey
2. 累加器
3. reidis

## 8，JUC和JVM面试题：

### 8.1 三句口诀

答：

> 1. 高内聚低耦合，线程操作资源类。
>
> 2. 判断/干活/通知。
>
> 3. 防虚假唤醒，用while进行判断。

### 8.2 Lock和synchronized区别

答：

1. Lock是一个java类（jvm层面的），synchronized是一个关键字（是api层面的）
2. Lock等待可以被打断，synchronized不会被打断(除非抛异常)
3. Lock需要手动释放锁(释放操作要放到finally里面)，synchronized的锁会自动释放
4. ReentrantLock可以精准唤醒某一个线程（一个Condition是一把锁），synchronized 不能控制唤醒某一个线程
5. 都是非公平锁

### 8.3 八锁机制

> **==结论一：==所有的非静态==同步==方法用的都是同一把锁——实例对象本身。**
>
> **==结论二：==对于静态==同步==方法，锁是当前类的Class对象。**
>
> **==结论三：==不管是否静态，对==非同步==方法都不上锁**。
>
> 1  一个对象里面如果有多个synchronized方法，某一个时刻内，只要一个线程去调用其中的一个synchronized方法了，其它的线程都只能等待，换句话说，某一个时刻内，只能有唯一一个线程去访问这些synchronized方法
>
> 2    锁的是当前对象this，被锁定后，其它的线程都不能进入到当前对象的其它的synchronized方法
>
> 3   加个普通方法后发现和同步锁无关
>
> 4   换成两个对象后，不是同一把锁了，情况立刻变化。



### 8.4 一个线程启动两次，就是调用两次start方法，会发生什么？

答：

**会发生异常IllegalThreadStateException**：线程状态不匹配异常。



### 8.5 JVM有几种类加载机制

答：三种系统类加载器和自定义加载器

1. 启动类加载器（bootstrap） C++
2. 拓展类加载器（Extension）Java
3. 应用程序类加载器（AppClassLoader）Java也加系统类加载器，加载当前应用的classpath的所有类
4. 用户自定义加载器（大神用）



### 8.6 双亲委派机制

答：

一个类收到类加载请求，会委派父加载器去加载，最后到启动类加载器，然后进行判断，如果启动类加载器加载不了，那就子类进行加载，（在它的加载路径下没有找到所需加载的Class）。



例：比如加载位于 rt.jar 包中的类 java.lang.Object。

终都是委托给顶层的启动类加载器进行加载，这样就保证了使用不同的类加载器最终得到的都是同样一个 Object对象。



### 8.7 jvm体系结构

答：

![](img\jvm体系结构.png)

### 8.8 类加载流程

答:

![](img\类加载流程.png)



### 8.9 堆内存有哪几个部分，怎么分配大小

答：

![](img\堆内存的结构图.png)



### 8.10 一个对象被new 出来经历了什么事情

答：

Person p = new Person();

> 1. ==**判读Person 有没有被加载到方法区里面，如果没有加载，先进行类的加载。**==
>
>    > 加载顺序：《client》类初始化：先加载父类的静态 -> 子类的静态属性 
>    >
>    > ​
>    >
>    > 加载方式：使用双亲委派机制。
>
> 2. ==**然后右边的new出来的实例对象放到堆内存里面的新年代的伊甸园区。**==
>
>    > new Person():
>    >
>    > ​	《init》实例初始化：调用构造方法，进行实例初始化。先加载父类的非静态 -> 子类的非静态属性 。
>    >
>    > ​
>    >
>    > 堆内存分配：
>    >
>    > 	1. 新年代：包括三个部分，伊甸园，幸存者0区，幸存者1区
>    > 	2. 老年代：
>    > 	3. 元空间：一般和方法区等同
>    >
>    > 堆内存GC：
>    >
>    > 1. 当伊甸园区的内存满了，会触发第一次GC，把没有回收掉的对象复制一份到from区，然后清空伊甸园区。当第二次伊甸园内存满了，会把伊甸园和from区没有回收掉的对象复制一份到to区，然后清空伊甸园区和from区，GC要符合谁空谁是TO，所有from和to进行交换，每次没有回收的对象年龄都会+1，当对象年龄到15岁就会放到老年代里面，
>    > 2. 老年代的内存如果满了，会触发fullGC，如果fullGC之后，老年代的对象回收不掉，会报异常，OutOfMemeryError异常。
>
> 3. ==**new 左边的是对象的引用，放到栈内存中。**==






##9，mysql高级部分：

### 9.1 说说mysql里面的事务

答：

事务就是一次数据库的连接，一个事务里面的操作要不全部失败，要不全部成功，保证数据的安全可靠。

**事务四大特性（ACID）：**

1. 原子性：事务操作不可分割，要不全部失败，要不全部成功。
2. 一致性：事务操作不会破坏数据库的完整性约束。
3. 隔离性：事务之间的操作互不影响。（和隔离级别有关系）
4. 持久性：事务操作完成之后，数据持久化后不会丢失，不会回滚。

**四大隔离级别：（线程安全问题）**

==**为了解决多线程并发访问mysql产生的线程安全问题，设置隔离级别来避免相关情况。**==

+ **脏读**: 对于两个事务 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段. 之后, 若 T2 回滚, T1读取的内容就是临时且无效的.
+ **不可重复读**: 对于两个事务 T1, T2, T1 读取了一个字段, 然后 T2 更新并提交了该字段. 之后, T1再次读取同一个字段, 值就不同了.
+  **幻读:** 对于两个事务 T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入、删除了一些新的行. 之后, 如果 T1 再次读取同一个表, 就会多出、少了几行.

| 隔离级别                                | 描述                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| READ-UNCOMMITTED未提交读                | 允许事务读取其他事务未提交的数据，脏读、不可重复读、幻读的问题都会出现 |
| READ-COMMITTED读已提交                  | 只允许事务读取其他事务已经提交的数据，可以避免脏读，但是不可重复读、幻读的问题仍然会出现 |
| REPEATABLE-READ可重复读（默认隔离级别） | 确保事务可以多次从一个字段中读取相同的值，好比在事务开启时对现有的数据进行了拍照，其他事务对数据的修改，不管事务是否提交，我这里读取的是拍照下来的数据，可以避免脏读和不可重复读，但幻读的问题仍然存在。注意：INNODB使用了MVCC (Multiversion Concurrency Control)，即多版本并发控制技术防止幻读。真正的像拍照一样，其他事务新插入或删除的记录也看不出来。 |
| SERIALIZABLE序列化                      | 确保事务可以从一个表中读取相同的行，在这个事务持续期间，禁止其他事务对该表执行插入、更新、删除操作，所有并发问题都可以避免，但是性能十分低下。 |

> ==**脏读，幻读，不可重复读**==
>
> **脏读：T1未提交，但是T2读到未提交的数据。**
>
> **幻读：T1已提交，但是T2读到了T1删除，添加的数据。**
>
> **不可重复读：T1已提交，但是T2读取到T1更新的数据。**

### 9.2 查看sql语句的执行计划

答：EXPLAIN select * from user_feedback;查看执行计划中的sql性能

### 9.2 mysql索引优化和索引失效情况

答：

**口诀：**

全职匹配我最爱，最左前缀要遵守；

带头大哥不能死，中间兄弟不能断；

索引列上少计算，范围之后全失效；

LIKE百分写最右，覆盖索引不写*；

不等空值还有OR，索引影响要注意；

VAR引号不可丢，SQL优化有诀窍。



### 9.2 B+树和B-/B树的区别

**B 树**

1.关键字集合分布在整颗树中；

2.任何一个关键字出现且只出现在一个结点中；

3.搜索有可能在非叶子结点结束；

4.其搜索性能等价于在关键字全集内做一次二分查找；

5.自动层次控制；

![](img\B树.png)



**B+树是对B树的一种变形树，它与B树的差异在于：**

1. 所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的；


2. 不可能在非叶子结点命中；


3. 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层；
4. 更适合文件索引系统；

![](img\B+树.png)

### 9.3 红黑树讲一下

答：

红黑树就是一种平衡的二叉查找树，说他平衡的意思是他不会变成“瘸子”，左腿特别长或者右腿特别长。除了符合二叉查找树的特性之外，还具体下列的特性：

1. 节点是红色或者黑色
2. 根节点是黑色
3. 每个叶子的节点都是黑色的空节点（NULL）
4. 每个红色节点的两个子节点都是黑色的。
5. 从任意节点到其每个叶子的所有路径都包含相同的黑色节点。

![](img\红黑树.png)

## 10，Redis部分：

### 10.1 redis五大数据结构

答：String，set，zset，list，hash。

### 10.2 redis的持久化方法，各有什么优缺点

答：有两种持久化方式：AOF和RDB

> ==**RDB**==
>
> Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到
> 一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件，形成新的快照文件。
>
> RDB的缺点:
>
> 1. 最后一次持久化后的数据可能丢失。
>
>
> 2. 虽然Redis在fork时使用了copyonwrite技术,但是如果数据庞大时还是比较消耗性能。
>
> RDB的优点：
>
> 1. 节约磁盘空间。
>
>
> 2. 恢复速度快。
>
> ==**AOF**==
>
> 以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)。
>
> AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，遍历新进程的内存中数据，每条记录有一条的Set语句。
>
> AOF的缺点：
>
>  	1. 比RDB占用更多的磁盘空间
> 	2. 恢复速度也较慢
>
> AOF的优点：
>
> 1. 备份机制更稳健，丢数据概率小（每秒或者每次写操作都会记录）
> 2. 日志文件可以修改，可以处理一些误操作
>
> ==**官方推荐两个都启用，默认开启的是RDB**==
>
> 

### 10.3 redis主从复制

答：

1. 读写分离，Master主要负责写，slave主要负责读。
2. 容灾快速恢复。
3. 配从不配主。

**复制原理：**

> 全量复制：Master接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，
> 在后台进程执行完毕之后，master将传送整个数据文件到slave,以完成一次完全同步，只要是重新连接master,一次完全同步将被自动执行一次全量复制。
>
> 增量复制：Master继续将新的所有收集到的修改命令依次传给slave,完成同步

###10.4 Redis哨兵模式:

> **哨兵只要是用来监控使用，当主机发生故障，要重新选择主机。**
>
> 1. 新主登基：
>
> > 1、选择优先级靠前的
> >
> > 2、选择偏移量最大的
> >
> > 3、选择runid最小的从服务。
>
> 2. 群仆俯首：
>
> > 挑选出新的主服务之后，新主向原主服务的从服务发送 slaveof 命令。
>
> 3. 旧主俯首：
>
> > 当之前的主机重新上线时，新主机会向旧主机发送slaveof命令，然后旧主变为仆机。
>
> 

### 10.5 Redis里面的事务(使用乐观锁)

答：

Redis事务特点：

1. 单独的隔离操作 
2. 没有隔离级别的概念：队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，也就不存在“事务内的查询能看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题 
3. 不保证原子性：redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚。



==redis事务里面采用乐观锁来实现：==

==乐观锁(Optimistic Lock),== 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于==多读==的应用类型，这样可以提高吞吐量，像数据库如果提供类似于check-and-set机制的其实都是提供的乐观锁。



==悲观锁(Pessimistic Lock),==顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如==行锁，表锁等，读锁，写锁==等，都是在做操作之前先上锁。



### 10.6 Redis在公司中用来干嘛？

答：

主要用做缓存：

1. 点击次数超过阈值，那么会将用户加入redis的黑名单中 

		2. top3 热门广告 
	3. 广告的点击流量实时统计

## 11，数仓项目：

### 11.1 项目中遇到的问题

1. **KafkaConsumer is not safe for multi-threaded**

https://blog.csdn.net/lmmzsn/article/details/78716824

https://blog.csdn.net/xianpanjia4616/article/details/82811414

==**报错信息：KafkaConsumer is not safe for multi-threaded**==

> KafkaConsumer多线程访问的时候,是不安全？
>
> **原因：因为Kafka consumer是非线程安全的,当多个线程同时共用一个consumer时会导致异常**
>
> **topic下的一个分区只能被同一个consumer group下的一个consumer线程来消费**
>
> ==对同一个RDD,使用了两次filter引起，rdd.filter{rdd.fliter},进行嵌套。==
>
> 而且线程 和 分区也有着限定的关系==**线程<=分区**==
>
> **解决1：把RDD先cache一下,然后后面对RDD的操作会从内存中去读RDD,这样就保证了kafka中的数据只被消费了一次,**
>
> **解决2：或者把线程数的连接数改小点，消费者连接数<=kafka分区数**

2. **启动kafaka失败了，提示zk保存kafka的ids已经存在，解决。**

（ A broker is already registered on the path /brokers/ids/1）

> ==**发现问题：kafka集群启动时，zk里面找不到该brokers_id，但是关闭kafka集群就可以发现这个brokers，我们只需要把ids这个节点删除就行，重新启动kafka，好了。**==
>
> 1. [推荐]您需要清理zookeeper路径/ brokers / ids / []中的代理ID。使用zk-cli tool delete命令清除路径。启动您的代理并验证它是否向协调员注册。
> 2. 解决此问题的另一种方法是从kafka服务器配置更改您的broker-id并重新启动代理。但是，这会破坏您的分区，不建议使用数据

## 12 大厂面试题(JUC/JVM)：

### 12.1 谈谈JMM是什么？

答：

JMM（java内存模型：抽象概念）：

1. 可见性
2. 原子性
3. 有序性（会发生指令重排）

![](E:img\JMM模型1.png)

![](E:img\JMM模型.png)

### 12.2 谈谈你对volatile的理解

答：volatile是jvm提供的轻量级的同步机制

> ==保证可见性==
>
> > **主内存数据共享，线程自己内存数据不共享，当要操作某个变量时，线程需要把变量从主内存复制一份放到自己的内存中，线程对变量的一系列操作完成之后，需要把副本变量放回到主内存。而且第一时间要保证其他线程可见。**
>
> ==不保证原子性==
>
> > **主内存数据共享，线程自己内存数据不共享，操作某个变量时，会出现写覆盖的情况，最后的结果和期望不一样。**
> >
> > 解决：
> >
> > ```scala
> > 1. 使用synchronized修饰
> > 2. ==使用JUC包下面的AtomicInteger(底层使用ACS)==
> > ```
> >
> > ==**面试题：多线程下面可以使用i++吗？**==
> >
> > **答： 不可以使用，线程不安全**
> >
> > **解决方法：使用atomicInteger.getAndIncrement();**
> >
> > 
>
> 
>
> ==禁止指令重排序==
>
> > **计算机在执行程序时，为了提高性能，编译器和处理器的常常会对指令做重排，一般分为以下3种**
> >
> > ==**源代码--> 编译器优化的重排--> 指令并行的重排--> 内存系统的重排--> 最终执行的指令**==
> >
> > 
> >
> > **单线程环境里面确保程序最终执行结果和代码顺序执行的结果一致。**
> >
> > 
> >
> > **处理器在进行重排序时必须要考虑指令之间的==数据依赖性==。**
> >
> > 
> >
> > **所线程环境中线程交替执行，由于编译器优化重排的存在，多个线程中使用的变量能否保证一致性是无法确定的，结果无法预测。**
> >
> > 

### 12.3 手写一个线程安全的单例设计模式(使用volatile)

答：

```java
class SimpleExample{
    private volatile static  SimpleExample instance = null;

    private SimpleExample(){
        System.out.println(Thread.currentThread().getName()+"单例设计模式");
    }
	
    //使用DCL（双端检锁）机制
    public static  SimpleExample getInstance(){
        if(instance == null){
            synchronized ("123"){
                if (instance == null){
                    instance = new SimpleExample();
                }
            }
        }
        return instance;
    }

    /**
     * 测试
     *
     */
    public static void main(String[] args){
        for(int i=0;i<10;i++){
            new Thread(()->{
                SimpleExample instance = SimpleExample.getInstance();
            },"线程"+i).start();
        }
    }
}
```

![](E:img\DCL.png)

但是指令重排只会保证串行语义的执行的一致性（单线程），但并不会关心多线程间的语义一致性。

所有当一条线程访问instance不为null时，由于instance实例未必已初始化完成，也就造成了线程安全问题。

### 12.4 CAS了解吗？

答：比较并交换（compareAndSwap）

> **如果线程的期望值和物理内存的真实值一样，就把变量修改为要更新的值。**
>
> **如果线程的期望值和物理内存的真实值不一样，本次修改操作失败。**
>
> ==**CAS底层原理：**==
>
> 1. ==Unsafe：==
>
> > CAS保证原子性，靠的是底层的Unsafe，来自于rt.java包里面。
> >
> > 1. ==Unsafe类里面的所有的方法都是native修饰的，也就是说Unsafe类中的方法都直接调用操作系统底层资源执行相应任务。==
> > 2. ==Unsafe就是根据内存偏移地址获取数据（valueOffset）。==
> > 3. ==变量值value用volatile修饰，保证了多线程之间的内存可见性。==
> > 4. ![](E:img\unsafe底层.png)
>
> 2. ==自旋锁：==
>
> > 

### 12.5 CAS缺点

答：

> 1. **循环时间开销大**
> 2. **只能保证一个共享变量的原子操作**
> 3. **出现ABA问题**

### 12.6 什么是ABA问题，怎么解决ABA问题

答：

**什么是ABA问题：**

> **比如说一个线程one从内存位置V中取出A，这个时候另一个线程two也从内存中取出A，并且线程two进行了一些操作将值变为了B，然后线程two又将V位置的数据变成A，这时候线程one进行CAS操作发现内存中仍然是A，然后线程one操作成功。**
>
> 
>
> **尽管线程one的CAS操作成功，但是不代表这个过程就是没有问题的。**

**解决ABA问题**：

> 1. 引入版本号机制，数据和版本号一致才算成功
> 2. ​



### 12.7 谈谈AtomicInteger，为啥用CAS而不用Synchronzied

答：

> **Synchronzied:：同一时间段，只允许一个线程进行访问，一致性得到了保证，但并发性下降。**
>
> 
>
> **CAS：底层源码使用do.....while进行CAS比较，直到比较成功为止。**
>
> 



## 手撕算法：

###二分法查找（并且分析时间和空间复杂度）：

说明：元素必须是==有序==的，如果是无序的则要先进行排序操作。

　　基本思想：也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。

　　**复杂度分析：满足公式$n=2^x$    ==>   $x=log2^n$时间复杂度为O($x=log2^n$)；**

​	**递归的次数和深度都是$log2^n$,每次所需要的辅助空间都是常数级别的：**

```java
package com.qyzx.search;

/**
 * 二分（折半）查找
 * @author Administrator
 *
 */
public class BinarySearch {

	public static void main(String[] args) {
		int[] arr = {1,2,3,4,5,6};
		int num = binary(arr,4);
		System.out.println(num);
	}

	private static int binary(int[] arr, int find) {
		if(arr==null){
			return -1;
		}
		
		int start = 0;
		int end = arr.length-1;
		//用来保存查找到的值的下标，如果没有，返回-1；
		int index = -1;
		int mid = start+(end-start)/2;
		while(start<=end){
			if(find == arr[mid]){//相等就直接返回
				index = mid;
				return index;
			}else if(find>arr[mid]){//如果查找的数比mid下标对应的值大的话，缩小范围，继续查找。
				start = mid+1;
			}else{//如果查找的数比mid下标对应的值大的话，缩小范围，继续查找。
				end = mid-1;
			}
			mid = start+(end-start)/2;
		}
		return index;
	}
}
```



###“ababfdegfd" 给定一个字符串，输出只出现一次的第一个元素，阐述一下时间复杂度

答：

**第一种：利用类hash表的方式来实现：数组下标为是元素的ASCII，存的值为元素出现的次数。**

```java
//利用数组的方式，定义一个255长度的数组
	public static char find1(String s){
		int[] tempArr = new int[255];
		
		//循环遍历字符串，然后保存到数组中，下标就是具体的字符，值就是count
		for(int i=0; i<s.length();i++){
			int c = s.charAt(i);
			tempArr[c]++;
		}
		
		//然后遍历判断，找出值count为1的字符
		for (int i = 0; i < s.length(); i++) {
			char c = s.charAt(i);
			//判断是否为1
			if(tempArr[c] == 1){
				char res = (char)c;
				return res;
			}
		}
		return 0;
	}
```

**第二种方法：利用hashMap来进行：(k,v)=>（元素，元素出现次数）**

```java
//利用hash表方式来进行操作
	public static char find2(String s){
		//定义一个hashMap,key保存具体字符，value保存count次数。
		HashMap<Character,Integer> tmpMap = new HashMap<>();
		
		//然后循环判断，判断出是否有相同的字符
		for (int i = 0; i < s.length(); i++) {
			if(tmpMap.containsKey(s.charAt(i))){
				int v = tmpMap.get(s.charAt(i));
				tmpMap.put(s.charAt(i), v+1);
			}else{
				tmpMap.put(s.charAt(i),1);
			}
		}
		
		//然后遍历，如果第一次出现的话，就返回出去
		for(int i=0;i<s.length();i++){
			if(tmpMap.get(s.charAt(i))==1){
				return s.charAt(i);
			}
		}
		return 0;
	}
```





##手撕wordCount和TopN：

